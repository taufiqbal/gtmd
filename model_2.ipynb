{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taufiqbal/gtmd/blob/main/model_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2H3wcI6BDBz",
        "outputId": "f487964c-cd6d-4db4-8531-a8f62dce35b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - loss: 32555.1855 - val_loss: 12223.2490\n",
            "Epoch 2/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 29805.5254 - val_loss: 11933.8320\n",
            "Epoch 3/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 27960.1641 - val_loss: 11504.5430\n",
            "Epoch 4/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 29065.0039 - val_loss: 10887.8311\n",
            "Epoch 5/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 23818.1914 - val_loss: 9950.0078\n",
            "Epoch 6/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 21863.9238 - val_loss: 8621.9678\n",
            "Epoch 7/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 21491.5312 - val_loss: 7141.6196\n",
            "Epoch 8/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 24685.4570 - val_loss: 6192.0815\n",
            "Epoch 9/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 20453.4863 - val_loss: 6015.9868\n",
            "Epoch 10/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 18662.1816 - val_loss: 6010.7085\n",
            "Epoch 11/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 19981.8379 - val_loss: 6027.8892\n",
            "Epoch 12/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 19800.8301 - val_loss: 6133.5376\n",
            "Epoch 13/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 19234.7871 - val_loss: 6245.5308\n",
            "Epoch 14/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 19115.8633 - val_loss: 6403.4512\n",
            "Epoch 15/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 21586.8594 - val_loss: 6468.6201\n",
            "Epoch 16/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 16671.4629 - val_loss: 6500.7773\n",
            "Epoch 17/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 14471.0947 - val_loss: 6332.3882\n",
            "Epoch 18/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 18977.1328 - val_loss: 6165.2441\n",
            "Epoch 19/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 15910.1602 - val_loss: 6159.7520\n",
            "Epoch 20/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 15745.8916 - val_loss: 6163.3618\n",
            "Epoch 21/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 14641.0664 - val_loss: 6108.4395\n",
            "Epoch 22/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 14454.6348 - val_loss: 6185.9355\n",
            "Epoch 23/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 14914.6699 - val_loss: 6158.2012\n",
            "Epoch 24/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 18350.1406 - val_loss: 6142.3198\n",
            "Epoch 25/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 22977.5254 - val_loss: 6265.5200\n",
            "Epoch 26/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 17993.0078 - val_loss: 6443.7905\n",
            "Epoch 27/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 15592.2549 - val_loss: 6516.3276\n",
            "Epoch 28/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 19575.5000 - val_loss: 6538.3184\n",
            "Epoch 29/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 14606.3271 - val_loss: 6413.8115\n",
            "Epoch 30/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 15455.5303 - val_loss: 6157.5713\n",
            "Epoch 31/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 17269.3301 - val_loss: 6009.5122\n",
            "Epoch 32/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 19429.7578 - val_loss: 5986.5996\n",
            "Epoch 33/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 18167.4160 - val_loss: 6079.4741\n",
            "Epoch 34/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 17283.9062 - val_loss: 6193.1870\n",
            "Epoch 35/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 15024.4863 - val_loss: 6170.2285\n",
            "Epoch 36/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 16490.2500 - val_loss: 6048.6484\n",
            "Epoch 37/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 17489.9023 - val_loss: 5970.2285\n",
            "Epoch 38/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 21227.3203 - val_loss: 5920.5103\n",
            "Epoch 39/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 18647.1836 - val_loss: 5956.3550\n",
            "Epoch 40/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 22199.0488 - val_loss: 6031.4229\n",
            "Epoch 41/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 14617.3057 - val_loss: 6132.2993\n",
            "Epoch 42/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 15263.5342 - val_loss: 6032.2134\n",
            "Epoch 43/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 15972.7139 - val_loss: 5931.7588\n",
            "Epoch 44/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 13035.8164 - val_loss: 5874.2188\n",
            "Epoch 45/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 13874.3369 - val_loss: 5843.8062\n",
            "Epoch 46/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 18467.1387 - val_loss: 5846.8496\n",
            "Epoch 47/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 19527.4219 - val_loss: 5894.3647\n",
            "Epoch 48/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 14560.7793 - val_loss: 5829.0435\n",
            "Epoch 49/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 17815.7129 - val_loss: 5835.4595\n",
            "Epoch 50/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 15503.4834 - val_loss: 5934.3335\n",
            "Epoch 51/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 14137.5859 - val_loss: 5933.7129\n",
            "Epoch 52/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 12388.6006 - val_loss: 5881.7349\n",
            "Epoch 53/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 19875.9395 - val_loss: 5805.7129\n",
            "Epoch 54/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 13002.6875 - val_loss: 5839.1333\n",
            "Epoch 55/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 19373.8594 - val_loss: 5950.8975\n",
            "Epoch 56/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 20679.3906 - val_loss: 5798.2236\n",
            "Epoch 57/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 14810.4092 - val_loss: 6226.1074\n",
            "Epoch 58/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 16857.9102 - val_loss: 6774.2754\n",
            "Epoch 59/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 22427.9941 - val_loss: 7018.7778\n",
            "Epoch 60/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 19655.8574 - val_loss: 7162.4858\n",
            "Epoch 61/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 17539.9980 - val_loss: 6964.9902\n",
            "Epoch 62/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 17141.8105 - val_loss: 6358.7769\n",
            "Epoch 63/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 18969.6055 - val_loss: 5993.4639\n",
            "Epoch 64/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 12802.0195 - val_loss: 5777.6011\n",
            "Epoch 65/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 19633.7852 - val_loss: 5732.1479\n",
            "Epoch 66/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 12481.3066 - val_loss: 5751.4214\n",
            "Epoch 67/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 15895.0527 - val_loss: 5803.2637\n",
            "Epoch 68/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 16104.2930 - val_loss: 5708.6621\n",
            "Epoch 69/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 15791.4316 - val_loss: 5913.9121\n",
            "Epoch 70/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 17592.7344 - val_loss: 6326.6333\n",
            "Epoch 71/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 19282.1699 - val_loss: 6684.6606\n",
            "Epoch 72/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 17624.0859 - val_loss: 6744.0054\n",
            "Epoch 73/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 19266.2637 - val_loss: 6577.0371\n",
            "Epoch 74/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 15148.2842 - val_loss: 6325.2534\n",
            "Epoch 75/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 12498.0586 - val_loss: 6041.3652\n",
            "Epoch 76/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 14350.9971 - val_loss: 5755.9277\n",
            "Epoch 77/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 13837.7588 - val_loss: 5647.1689\n",
            "Epoch 78/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 17146.5117 - val_loss: 5618.7319\n",
            "Epoch 79/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 17976.4492 - val_loss: 5709.5234\n",
            "Epoch 80/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 11928.0967 - val_loss: 5812.6904\n",
            "Epoch 81/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 17563.6621 - val_loss: 5913.3071\n",
            "Epoch 82/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 14452.5791 - val_loss: 5860.3730\n",
            "Epoch 83/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13039.3340 - val_loss: 5720.5869\n",
            "Epoch 84/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 20863.0039 - val_loss: 5748.5044\n",
            "Epoch 85/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 17152.1074 - val_loss: 5870.1763\n",
            "Epoch 86/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 18374.7188 - val_loss: 6044.3872\n",
            "Epoch 87/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 13441.0322 - val_loss: 6286.1128\n",
            "Epoch 88/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 13684.6299 - val_loss: 6362.8687\n",
            "Epoch 89/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 14493.3047 - val_loss: 6206.8257\n",
            "Epoch 90/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 13167.8564 - val_loss: 6043.0513\n",
            "Epoch 91/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13637.0352 - val_loss: 5832.0493\n",
            "Epoch 92/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 14701.8037 - val_loss: 5698.8145\n",
            "Epoch 93/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 15087.3887 - val_loss: 5619.3682\n",
            "Epoch 94/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 18826.3223 - val_loss: 5647.0986\n",
            "Epoch 95/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 15149.0977 - val_loss: 5781.7759\n",
            "Epoch 96/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 14915.1680 - val_loss: 5645.9009\n",
            "Epoch 97/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 16528.0488 - val_loss: 5752.3975\n",
            "Epoch 98/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 18817.4336 - val_loss: 6162.1001\n",
            "Epoch 99/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 15877.2871 - val_loss: 6454.6582\n",
            "Epoch 100/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 16808.3340 - val_loss: 6266.1719\n",
            "Epoch 101/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 13924.9375 - val_loss: 5743.3022\n",
            "Epoch 102/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 13932.9131 - val_loss: 5473.2202\n",
            "Epoch 103/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 18289.3672 - val_loss: 5595.1855\n",
            "Epoch 104/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 19478.8887 - val_loss: 5492.1445\n",
            "Epoch 105/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 15882.7686 - val_loss: 5476.5068\n",
            "Epoch 106/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 18514.1094 - val_loss: 5822.1206\n",
            "Epoch 107/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 15159.9355 - val_loss: 6174.1587\n",
            "Epoch 108/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 16292.8877 - val_loss: 6413.6631\n",
            "Epoch 109/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 15250.0264 - val_loss: 6646.9849\n",
            "Epoch 110/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 18424.8691 - val_loss: 6552.6650\n",
            "Epoch 111/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 16827.7617 - val_loss: 6367.5020\n",
            "Epoch 112/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 15700.9893 - val_loss: 5995.1543\n",
            "Epoch 113/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 14607.3555 - val_loss: 5631.8281\n",
            "Epoch 114/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 16395.8164 - val_loss: 5459.7300\n",
            "Epoch 115/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 24828.5625 - val_loss: 5500.9600\n",
            "Epoch 116/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 14238.4590 - val_loss: 5657.5508\n",
            "Epoch 117/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 12695.9043 - val_loss: 5781.4189\n",
            "Epoch 118/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 14753.1680 - val_loss: 5840.8921\n",
            "Epoch 119/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 12877.8203 - val_loss: 5734.8066\n",
            "Epoch 120/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 12055.5010 - val_loss: 5535.9658\n",
            "Epoch 121/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 11379.6934 - val_loss: 5502.8042\n",
            "Epoch 122/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 12681.7715 - val_loss: 5518.4434\n",
            "Epoch 123/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 13758.6260 - val_loss: 5524.1328\n",
            "Epoch 124/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 12746.9043 - val_loss: 5535.1328\n",
            "Epoch 125/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 16308.8369 - val_loss: 5409.3730\n",
            "Epoch 126/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 16956.3613 - val_loss: 5363.9478\n",
            "Epoch 127/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 16215.0391 - val_loss: 5397.5708\n",
            "Epoch 128/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 14338.1348 - val_loss: 5588.6567\n",
            "Epoch 129/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 13360.6924 - val_loss: 5857.2510\n",
            "Epoch 130/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 14654.2598 - val_loss: 5960.6265\n",
            "Epoch 131/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 12549.9229 - val_loss: 5958.7148\n",
            "Epoch 132/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 13401.1875 - val_loss: 5765.3364\n",
            "Epoch 133/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 15531.2520 - val_loss: 5495.1997\n",
            "Epoch 134/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 12953.9648 - val_loss: 5320.6025\n",
            "Epoch 135/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 17634.7129 - val_loss: 5339.7397\n",
            "Epoch 136/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 17148.8555 - val_loss: 5386.3179\n",
            "Epoch 137/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 14698.6670 - val_loss: 5460.5728\n",
            "Epoch 138/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 14037.5166 - val_loss: 5566.8550\n",
            "Epoch 139/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 15555.1270 - val_loss: 5949.8208\n",
            "Epoch 140/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 13428.3936 - val_loss: 5893.9946\n",
            "Epoch 141/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 24599.3047 - val_loss: 5924.9863\n",
            "Epoch 142/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 16129.0898 - val_loss: 5934.9463\n",
            "Epoch 143/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 12761.7441 - val_loss: 5903.5649\n",
            "Epoch 144/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 12737.3438 - val_loss: 5784.6416\n",
            "Epoch 145/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 15021.5322 - val_loss: 5583.1450\n",
            "Epoch 146/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 16608.1895 - val_loss: 5590.4521\n",
            "Epoch 147/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 13811.8145 - val_loss: 5684.6440\n",
            "Epoch 148/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 15813.8604 - val_loss: 6117.2280\n",
            "Epoch 149/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 12291.2910 - val_loss: 6996.4053\n",
            "Epoch 150/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 14489.5566 - val_loss: 6999.1753\n",
            "Epoch 151/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 18281.1426 - val_loss: 6580.1777\n",
            "Epoch 152/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 11076.2305 - val_loss: 5904.8755\n",
            "Epoch 153/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 17141.4043 - val_loss: 5412.9277\n",
            "Epoch 154/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 12944.8193 - val_loss: 5326.7793\n",
            "Epoch 155/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 16022.1465 - val_loss: 5329.2559\n",
            "Epoch 156/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 17816.9121 - val_loss: 5444.7319\n",
            "Epoch 157/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 13555.7725 - val_loss: 5765.6479\n",
            "Epoch 158/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13960.4277 - val_loss: 5971.5254\n",
            "Epoch 159/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 14327.1279 - val_loss: 6128.7500\n",
            "Epoch 160/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 17943.2852 - val_loss: 6020.9648\n",
            "Epoch 161/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 12450.2607 - val_loss: 5831.4697\n",
            "Epoch 162/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 13806.4170 - val_loss: 5518.8560\n",
            "Epoch 163/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 14499.3838 - val_loss: 5554.2090\n",
            "Epoch 164/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 15195.5869 - val_loss: 5467.8950\n",
            "Epoch 165/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 15686.6816 - val_loss: 5578.7339\n",
            "Epoch 166/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 13784.1611 - val_loss: 5569.1206\n",
            "Epoch 167/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13673.5615 - val_loss: 5817.1846\n",
            "Epoch 168/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 10697.5000 - val_loss: 5830.9795\n",
            "Epoch 169/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 15784.9463 - val_loss: 5777.4995\n",
            "Epoch 170/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 19170.3203 - val_loss: 5892.5435\n",
            "Epoch 171/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 13085.8320 - val_loss: 6033.0601\n",
            "Epoch 172/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13840.2070 - val_loss: 5766.8608\n",
            "Epoch 173/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 11455.2070 - val_loss: 5516.2524\n",
            "Epoch 174/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 11176.9141 - val_loss: 5369.2510\n",
            "Epoch 175/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 11656.8164 - val_loss: 5250.3242\n",
            "Epoch 176/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 14089.9912 - val_loss: 5315.5117\n",
            "Epoch 177/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 17655.1191 - val_loss: 5456.2773\n",
            "Epoch 178/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 15829.6318 - val_loss: 5585.2847\n",
            "Epoch 179/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 16194.8066 - val_loss: 5639.6870\n",
            "Epoch 180/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 12880.1631 - val_loss: 5554.7129\n",
            "Epoch 181/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 17171.7793 - val_loss: 5357.2632\n",
            "Epoch 182/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 11801.4238 - val_loss: 5193.5884\n",
            "Epoch 183/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 14437.4785 - val_loss: 5161.0420\n",
            "Epoch 184/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 14642.5361 - val_loss: 5303.8965\n",
            "Epoch 185/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 14811.5234 - val_loss: 5491.3276\n",
            "Epoch 186/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 16649.0137 - val_loss: 5711.4600\n",
            "Epoch 187/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13608.4844 - val_loss: 5800.2598\n",
            "Epoch 188/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 12409.3203 - val_loss: 5732.3999\n",
            "Epoch 189/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 15460.1367 - val_loss: 5627.4580\n",
            "Epoch 190/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 12923.3232 - val_loss: 5429.1338\n",
            "Epoch 191/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 16585.6016 - val_loss: 5499.7988\n",
            "Epoch 192/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 16424.4492 - val_loss: 5568.5024\n",
            "Epoch 193/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 11519.2754 - val_loss: 5564.0933\n",
            "Epoch 194/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 12196.9590 - val_loss: 5401.6431\n",
            "Epoch 195/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 21307.4023 - val_loss: 5370.2212\n",
            "Epoch 196/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 15600.6426 - val_loss: 5414.0708\n",
            "Epoch 197/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 14042.7070 - val_loss: 5552.9595\n",
            "Epoch 198/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13210.8457 - val_loss: 5625.5000\n",
            "Epoch 199/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13704.9736 - val_loss: 5478.0938\n",
            "Epoch 200/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 20191.2266 - val_loss: 5495.6123\n",
            "Epoch 201/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13821.7051 - val_loss: 5658.5439\n",
            "Epoch 202/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 12020.5293 - val_loss: 5912.5737\n",
            "Epoch 203/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 14821.7295 - val_loss: 6163.9873\n",
            "Epoch 204/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13597.9492 - val_loss: 6356.5508\n",
            "Epoch 205/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 12905.4580 - val_loss: 6340.0972\n",
            "Epoch 206/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 13768.0967 - val_loss: 6152.6318\n",
            "Epoch 207/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 16126.9316 - val_loss: 5712.5454\n",
            "Epoch 208/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 14688.4941 - val_loss: 5386.0513\n",
            "Epoch 209/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 20022.8008 - val_loss: 5469.0371\n",
            "Epoch 210/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 14427.9355 - val_loss: 5677.5088\n",
            "Epoch 211/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 11920.9814 - val_loss: 5737.1792\n",
            "Epoch 212/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 12069.5225 - val_loss: 5469.3638\n",
            "Epoch 213/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 17150.4688 - val_loss: 5600.8843\n",
            "Epoch 214/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 12057.4668 - val_loss: 5781.5137\n",
            "Epoch 215/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 13381.0977 - val_loss: 5962.2754\n",
            "Epoch 216/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 16973.9141 - val_loss: 6263.2168\n",
            "Epoch 217/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 16724.9492 - val_loss: 6356.1650\n",
            "Epoch 218/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 14216.7139 - val_loss: 6091.2837\n",
            "Epoch 219/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 13269.5879 - val_loss: 5735.9653\n",
            "Epoch 220/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 13516.4688 - val_loss: 5601.1582\n",
            "Epoch 221/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 15851.0605 - val_loss: 5575.7935\n",
            "Epoch 222/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 13756.5625 - val_loss: 5621.7666\n",
            "Epoch 223/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 17771.1504 - val_loss: 5882.1030\n",
            "Epoch 224/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 15077.7637 - val_loss: 6116.8115\n",
            "Epoch 225/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 13019.2900 - val_loss: 5963.4902\n",
            "Epoch 226/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 16026.8184 - val_loss: 5734.1675\n",
            "Epoch 227/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 13445.0195 - val_loss: 5471.2207\n",
            "Epoch 228/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 12524.1758 - val_loss: 5452.0107\n",
            "Epoch 229/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 12364.7178 - val_loss: 5722.8149\n",
            "Epoch 230/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 14636.2607 - val_loss: 6326.3188\n",
            "Epoch 231/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 14774.0059 - val_loss: 6980.9976\n",
            "Epoch 232/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 12837.9365 - val_loss: 6892.5225\n",
            "Epoch 233/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 15245.0566 - val_loss: 6471.7397\n",
            "Epoch 234/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 15484.7832 - val_loss: 5979.9082\n",
            "Epoch 235/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 16034.0977 - val_loss: 5415.1055\n",
            "Epoch 236/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 13189.6611 - val_loss: 5106.2319\n",
            "Epoch 237/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 13677.8809 - val_loss: 5056.7979\n",
            "Epoch 238/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 16723.7148 - val_loss: 5203.4614\n",
            "Epoch 239/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 16076.5771 - val_loss: 5376.6162\n",
            "Epoch 240/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 10745.9346 - val_loss: 5701.3359\n",
            "Epoch 241/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 12951.7012 - val_loss: 5666.9526\n",
            "Epoch 242/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 10465.0078 - val_loss: 5403.5991\n",
            "Epoch 243/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 13819.7373 - val_loss: 5047.0332\n",
            "Epoch 244/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 13888.2256 - val_loss: 5165.2251\n",
            "Epoch 245/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 15783.1357 - val_loss: 5561.9663\n",
            "Epoch 246/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 11609.0303 - val_loss: 6009.1226\n",
            "Epoch 247/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 12568.8105 - val_loss: 6356.4482\n",
            "Epoch 248/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 13065.4033 - val_loss: 6848.6899\n",
            "Epoch 249/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 16671.4805 - val_loss: 6754.8794\n",
            "Epoch 250/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 14359.7461 - val_loss: 6207.5322\n",
            "Epoch 251/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 12225.2861 - val_loss: 5694.5327\n",
            "Epoch 252/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 14511.9258 - val_loss: 5293.1626\n",
            "Epoch 253/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 14879.6758 - val_loss: 5092.6753\n",
            "Epoch 254/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 17168.8867 - val_loss: 5197.9399\n",
            "Epoch 255/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 13427.0625 - val_loss: 5579.1143\n",
            "Epoch 256/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 15847.9492 - val_loss: 6120.4351\n",
            "Epoch 257/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 14991.6641 - val_loss: 6520.5356\n",
            "Epoch 258/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 12622.0801 - val_loss: 6595.5444\n",
            "Epoch 259/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 14999.9404 - val_loss: 6332.2632\n",
            "Epoch 260/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 16427.5723 - val_loss: 5715.5483\n",
            "Epoch 261/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 13554.4844 - val_loss: 5307.8750\n",
            "Epoch 262/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 15066.2275 - val_loss: 5213.5918\n",
            "Epoch 263/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 14982.5430 - val_loss: 5599.1787\n",
            "Epoch 264/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 15035.8242 - val_loss: 5806.7705\n",
            "Epoch 265/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 17830.5977 - val_loss: 5930.1807\n",
            "Epoch 266/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 14943.5635 - val_loss: 5862.1519\n",
            "Epoch 267/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 12933.4727 - val_loss: 5754.2588\n",
            "Epoch 268/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 10199.2861 - val_loss: 5498.5879\n",
            "Epoch 269/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 15691.2021 - val_loss: 5103.1860\n",
            "Epoch 270/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 16236.7109 - val_loss: 5017.5591\n",
            "Epoch 271/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 19833.1758 - val_loss: 5042.2051\n",
            "Epoch 272/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 16886.4238 - val_loss: 5202.9185\n",
            "Epoch 273/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 11159.2129 - val_loss: 5425.4536\n",
            "Epoch 274/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 17774.1934 - val_loss: 5689.3647\n",
            "Epoch 275/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 12701.7236 - val_loss: 5890.1123\n",
            "Epoch 276/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 16361.4180 - val_loss: 6139.7188\n",
            "Epoch 277/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 11155.0859 - val_loss: 6084.7007\n",
            "Epoch 278/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 12156.9883 - val_loss: 5682.5776\n",
            "Epoch 279/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 12992.4902 - val_loss: 5461.3921\n",
            "Epoch 280/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 11593.7588 - val_loss: 5235.3901\n",
            "Epoch 281/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 11225.1592 - val_loss: 5215.9355\n",
            "Epoch 282/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 15177.0732 - val_loss: 5289.7773\n",
            "Epoch 283/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 11448.2539 - val_loss: 5322.3291\n",
            "Epoch 284/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 12627.5840 - val_loss: 5195.0562\n",
            "Epoch 285/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 13282.8203 - val_loss: 5639.0840\n",
            "Epoch 286/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 15382.0586 - val_loss: 6265.6294\n",
            "Epoch 287/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 10523.5391 - val_loss: 6697.9966\n",
            "Epoch 288/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 14850.0684 - val_loss: 6250.8730\n",
            "Epoch 289/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 17529.5352 - val_loss: 5763.2476\n",
            "Epoch 290/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 17092.2402 - val_loss: 5442.6035\n",
            "Epoch 291/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 18873.3691 - val_loss: 5642.4502\n",
            "Epoch 292/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 18736.0625 - val_loss: 5810.7041\n",
            "Epoch 293/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 13819.4746 - val_loss: 5727.1157\n",
            "Epoch 294/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 11248.9580 - val_loss: 5279.0645\n",
            "Epoch 295/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 12677.3789 - val_loss: 5085.7881\n",
            "Epoch 296/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 13051.1016 - val_loss: 5043.7017\n",
            "Epoch 297/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 11882.2891 - val_loss: 5177.1973\n",
            "Epoch 298/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 11633.3604 - val_loss: 5199.9985\n",
            "Epoch 299/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 11440.5156 - val_loss: 5807.8115\n",
            "Epoch 300/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 15707.0127 - val_loss: 6068.4634\n",
            "Epoch 301/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 12425.7393 - val_loss: 5926.0566\n",
            "Epoch 302/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 14645.8672 - val_loss: 5643.1270\n",
            "Epoch 303/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 15372.2568 - val_loss: 5324.1357\n",
            "Epoch 304/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 11253.9980 - val_loss: 5208.3691\n",
            "Epoch 305/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 13246.7764 - val_loss: 4906.7607\n",
            "Epoch 306/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 12002.4756 - val_loss: 4909.6533\n",
            "Epoch 307/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 16874.3809 - val_loss: 5133.0605\n",
            "Epoch 308/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 20503.0723 - val_loss: 5663.5801\n",
            "Epoch 309/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 12756.8457 - val_loss: 6273.7070\n",
            "Epoch 310/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 16960.6992 - val_loss: 6428.7588\n",
            "Epoch 311/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 11993.7334 - val_loss: 6251.1909\n",
            "Epoch 312/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 13562.0391 - val_loss: 5878.0107\n",
            "Epoch 313/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 11704.9580 - val_loss: 5428.0020\n",
            "Epoch 314/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 10201.1660 - val_loss: 5104.0269\n",
            "Epoch 315/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 17102.4121 - val_loss: 5044.0889\n",
            "Epoch 316/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 12817.7168 - val_loss: 5153.5488\n",
            "Epoch 317/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 11709.1406 - val_loss: 5106.8286\n",
            "Epoch 318/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 12347.3154 - val_loss: 5371.5835\n",
            "Epoch 319/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 11036.8105 - val_loss: 5485.0796\n",
            "Epoch 320/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 11897.8760 - val_loss: 5324.5713\n",
            "Epoch 321/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 13699.8887 - val_loss: 5732.1499\n",
            "Epoch 322/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 12501.4316 - val_loss: 6185.3735\n",
            "Epoch 323/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 12650.5723 - val_loss: 6274.3818\n",
            "Epoch 324/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 11561.7471 - val_loss: 6178.3037\n",
            "Epoch 325/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 10480.5059 - val_loss: 5818.1118\n",
            "Epoch 326/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 13053.7715 - val_loss: 5512.2520\n",
            "Epoch 327/1000\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 15981.5576 - val_loss: 5075.7168\n",
            "Epoch 328/1000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Membaca data dari file CSV\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/jpsi/fix_data_all.txt'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Memisahkan fitur input (Q^2, W, t) dan target output (sigma)\n",
        "X = data[['Q^2', 'W', 't']].values\n",
        "y = data['sigma'].values\n",
        "error_bar = data['error_bar'].values\n",
        "\n",
        "# Membagi data menjadi training dan testing set, termasuk error bar\n",
        "X_train, X_test, y_train, y_test, error_bar_train, error_bar_test = train_test_split(\n",
        "    X, y, error_bar, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Membangun model neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(3,)))  # Layer input dengan 3 fitur\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))                                    # Layer dropout untuk regularization\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))                                    # Layer dropout untuk regularization\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))                   # Layer output untuk prediksi sigma\n",
        "\n",
        "# Menggunakan MSE sebagai loss function\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Training model\n",
        "history = model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Evaluasi model pada data testing\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f'Loss on test data: {loss}')\n",
        "\n",
        "# Prediksi menggunakan model\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Menampilkan hasil prediksi bersamaan dengan data aktual\n",
        "results_df = pd.DataFrame({\n",
        "    'Q^2': X_test[:, 0],\n",
        "    'W': X_test[:, 1],\n",
        "    't': X_test[:, 2],\n",
        "    'Actual Sigma': y_test,\n",
        "    'Predicted Sigma': predictions.flatten(),\n",
        "    'Error Bar': error_bar_test  # Menggunakan error bar dari set test yang benar\n",
        "})\n",
        "\n",
        "print(results_df)  # Menampilkan hasil prediksi dan data aktual\n",
        "\n",
        "# Plotting training & validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA1z4aVLio2p",
        "outputId": "2f24dae3-033e-4a19-9290-2bb497384b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.00e-02 9.00e+01 3.00e-02]\n",
            " [5.00e-02 9.00e+01 1.00e-01]\n",
            " [5.00e-02 9.00e+01 1.70e-01]\n",
            " [5.00e-02 9.00e+01 2.50e-01]\n",
            " [5.00e-02 9.00e+01 3.50e-01]\n",
            " [5.00e-02 9.00e+01 4.90e-01]\n",
            " [5.00e-02 9.00e+01 7.30e-01]\n",
            " [5.00e-02 9.00e+01 1.03e+00]\n",
            " [3.20e+00 9.00e+01 4.00e-02]\n",
            " [3.20e+00 9.00e+01 1.30e-01]\n",
            " [3.20e+00 9.00e+01 2.70e-01]\n",
            " [3.20e+00 9.00e+01 6.80e-01]\n",
            " [7.50e+00 9.00e+01 4.00e-02]\n",
            " [7.50e+00 9.00e+01 1.30e-01]\n",
            " [7.50e+00 9.00e+01 2.70e-01]\n",
            " [7.50e+00 9.00e+01 6.80e-01]\n",
            " [2.24e+01 9.00e+01 4.00e-02]\n",
            " [2.24e+01 9.00e+01 1.30e-01]\n",
            " [2.24e+01 9.00e+01 2.70e-01]\n",
            " [2.24e+01 9.00e+01 6.80e-01]\n",
            " [3.10e+00 9.00e+01 5.00e-02]\n",
            " [3.10e+00 9.00e+01 1.50e-01]\n",
            " [3.10e+00 9.00e+01 2.90e-01]\n",
            " [3.10e+00 9.00e+01 5.80e-01]\n",
            " [6.80e+00 9.00e+01 5.00e-02]\n",
            " [6.80e+00 9.00e+01 1.50e-01]\n",
            " [6.80e+00 9.00e+01 2.90e-01]\n",
            " [6.80e+00 9.00e+01 5.80e-01]\n",
            " [6.80e+00 9.00e+01 5.00e-02]\n",
            " [6.80e+00 9.00e+01 1.50e-01]\n",
            " [6.80e+00 9.00e+01 2.90e-01]\n",
            " [6.80e+00 9.00e+01 5.80e-01]\n",
            " [1.60e+01 9.00e+01 5.00e-02]\n",
            " [1.60e+01 9.00e+01 1.50e-01]\n",
            " [1.60e+01 9.00e+01 2.90e-01]\n",
            " [1.60e+01 9.00e+01 5.80e-01]\n",
            " [5.00e-02 4.50e+01 3.00e-02]\n",
            " [5.00e-02 5.50e+01 3.00e-02]\n",
            " [5.00e-02 6.50e+01 3.00e-02]\n",
            " [5.00e-02 7.50e+01 3.00e-02]\n",
            " [5.00e-02 8.50e+01 3.00e-02]\n",
            " [5.00e-02 9.50e+01 3.00e-02]\n",
            " [5.00e-02 1.05e+02 3.00e-02]\n",
            " [5.00e-02 1.19e+02 3.00e-02]\n",
            " [5.00e-02 1.44e+02 3.00e-02]\n",
            " [5.00e-02 1.81e+02 3.00e-02]\n",
            " [5.00e-02 2.51e+02 3.00e-02]\n",
            " [5.00e-02 4.50e+01 1.00e-01]\n",
            " [5.00e-02 5.50e+01 1.00e-01]\n",
            " [5.00e-02 6.50e+01 1.00e-01]\n",
            " [5.00e-02 7.50e+01 1.00e-01]\n",
            " [5.00e-02 8.50e+01 1.00e-01]\n",
            " [5.00e-02 9.50e+01 1.00e-01]\n",
            " [5.00e-02 1.05e+02 1.00e-01]\n",
            " [5.00e-02 1.19e+02 1.00e-01]\n",
            " [5.00e-02 1.44e+02 1.00e-01]\n",
            " [5.00e-02 1.81e+02 1.00e-01]\n",
            " [5.00e-02 2.51e+02 1.00e-01]\n",
            " [5.00e-02 4.50e+01 2.20e-01]\n",
            " [5.00e-02 5.50e+01 2.20e-01]\n",
            " [5.00e-02 6.50e+01 2.20e-01]\n",
            " [5.00e-02 7.50e+01 2.20e-01]\n",
            " [5.00e-02 8.50e+01 2.20e-01]\n",
            " [5.00e-02 9.50e+01 2.20e-01]\n",
            " [5.00e-02 1.05e+02 2.20e-01]\n",
            " [5.00e-02 1.19e+02 2.20e-01]\n",
            " [5.00e-02 1.44e+02 2.20e-01]\n",
            " [5.00e-02 1.81e+02 2.20e-01]\n",
            " [5.00e-02 2.51e+02 2.20e-01]\n",
            " [5.00e-02 4.50e+01 4.30e-01]\n",
            " [5.00e-02 5.50e+01 4.30e-01]\n",
            " [5.00e-02 6.50e+01 4.30e-01]\n",
            " [5.00e-02 7.50e+01 4.30e-01]\n",
            " [5.00e-02 8.50e+01 4.30e-01]\n",
            " [5.00e-02 9.50e+01 4.30e-01]\n",
            " [5.00e-02 1.05e+02 4.30e-01]\n",
            " [5.00e-02 1.19e+02 4.30e-01]\n",
            " [5.00e-02 1.44e+02 4.30e-01]\n",
            " [5.00e-02 1.81e+02 4.30e-01]\n",
            " [5.00e-02 2.51e+02 4.30e-01]\n",
            " [5.00e-02 4.50e+01 8.30e-01]\n",
            " [5.00e-02 5.50e+01 8.30e-01]\n",
            " [5.00e-02 6.50e+01 8.30e-01]\n",
            " [5.00e-02 7.50e+01 8.30e-01]\n",
            " [5.00e-02 8.50e+01 8.30e-01]\n",
            " [5.00e-02 9.50e+01 8.30e-01]\n",
            " [5.00e-02 1.05e+02 8.30e-01]\n",
            " [5.00e-02 1.19e+02 8.30e-01]\n",
            " [5.00e-02 1.44e+02 8.30e-01]\n",
            " [5.00e-02 1.81e+02 8.30e-01]\n",
            " [5.00e-02 2.51e+02 8.30e-01]\n",
            " [8.90e+00 5.70e+01 5.00e-02]\n",
            " [8.90e+00 9.80e+01 5.00e-02]\n",
            " [8.90e+00 1.40e+02 5.00e-02]\n",
            " [8.90e+00 5.70e+01 1.90e-01]\n",
            " [8.90e+00 9.80e+01 1.90e-01]\n",
            " [8.90e+00 1.40e+02 1.90e-01]\n",
            " [8.90e+00 5.70e+01 6.40e-01]\n",
            " [8.90e+00 9.80e+01 6.40e-01]\n",
            " [8.90e+00 1.40e+02 6.40e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLkvT-KfiwMw",
        "outputId": "d252fdd7-6eed-40ca-d165-f9053cf4cb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[285.   180.   130.    92.1   61.2   32.5   10.6    2.7  107.    95.1\n",
            "  40.2    8.04  78.6   27.7   18.9    5.21  15.     8.9    4.55   1.36\n",
            " 148.    86.9   49.2   10.7   79.2   43.9   25.8    6.    75.6   39.6\n",
            "  23.9    6.5   28.    15.6    9.4    1.8  182.   208.   225.   321.\n",
            " 292.   326.   392.   376.   458.   537.   744.   115.   118.   169.\n",
            " 151.   178.   224.   226.   265.   267.   427.   573.    64.9   69.6\n",
            " 107.1   93.4  132.   135.   125.   142.   167.   202.   246.    35.5\n",
            "  35.6   34.3   38.9   41.4   46.3   48.5   60.9   51.5   67.    71.6\n",
            "   5.7    5.5    6.2    7.1    8.2    7.5    7.7    8.2    8.4   10.7\n",
            "  13.1   33.3   51.3   60.    17.3   30.1   31.5    3.4    5.5    6.  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSTraa2SihMD",
        "outputId": "44f5f4cc-6acb-4e47-cb15-37969e459eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.00e-02 7.50e+01 8.30e-01]\n",
            " [5.00e-02 1.05e+02 1.00e-01]\n",
            " [5.00e-02 5.50e+01 4.30e-01]\n",
            " [5.00e-02 1.81e+02 3.00e-02]\n",
            " [5.00e-02 1.44e+02 3.00e-02]\n",
            " [5.00e-02 7.50e+01 3.00e-02]\n",
            " [3.10e+00 9.00e+01 2.90e-01]\n",
            " [5.00e-02 4.50e+01 8.30e-01]\n",
            " [3.20e+00 9.00e+01 2.70e-01]\n",
            " [5.00e-02 9.00e+01 3.00e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR7bS3cogJ9Z",
        "outputId": "53d4ee2a-a7c8-40c7-b615-6fcae9420697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  7.1 226.   35.6 537.  458.  321.   49.2   5.7  40.2 285. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaoL2iFfh5Xe",
        "outputId": "5d50ae13-f6aa-4c3f-f344-da211523e620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Q^2      W     t  sigma  error_bar\n",
            "0   0.05   90.0  0.03  285.0      26.57\n",
            "1   0.05   90.0  0.10  180.0      17.46\n",
            "2   0.05   90.0  0.17  130.0      12.53\n",
            "3   0.05   90.0  0.25   92.1       9.03\n",
            "4   0.05   90.0  0.35   61.2       6.23\n",
            "..   ...    ...   ...    ...        ...\n",
            "95  8.90   98.0  0.19   30.1       4.55\n",
            "96  8.90  140.0  0.19   31.5       6.53\n",
            "97  8.90   57.0  0.64    3.4       0.58\n",
            "98  8.90   98.0  0.64    5.5       0.86\n",
            "99  8.90  140.0  0.64    6.0       1.25\n",
            "\n",
            "[100 rows x 5 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RLyvsmCu0xV26wr9hD1KegmwfLfbbJs8",
      "authorship_tag": "ABX9TyP+r4nNFjfIyGZ8UyWVvmvL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}